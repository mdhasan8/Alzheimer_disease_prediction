{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Imports and constants","metadata":{}},{"cell_type":"code","source":"import sys\nimport os\nimport random\nimport collections\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torchvision\nfrom torchvision.transforms import ToPILImage\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n\nfrom PIL import Image, ImageFile\n\n\n# We only use 3 transformations from this package\nsys.path.append(\"../input/maskrcnn-utils/\")\nfrom transforms import ToTensor, RandomHorizontalFlip, Compose\n\n\ndef fix_all_seeds(seed):\n    np.random.seed(seed)\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    \nfix_all_seeds(2021)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-10-20T14:28:57.343211Z","iopub.execute_input":"2021-10-20T14:28:57.343834Z","iopub.status.idle":"2021-10-20T14:28:57.897877Z","shell.execute_reply.started":"2021-10-20T14:28:57.343738Z","shell.execute_reply":"2021-10-20T14:28:57.897049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SAMPLE_SUBMISSION  = '../input/sartorius-cell-instance-segmentation/sample_submission.csv'\nTRAIN_CSV = \"../input/sartorius-cell-instance-segmentation/train.csv\"\nTRAIN_PATH = \"../input/sartorius-cell-instance-segmentation/train\"\nTEST_PATH = \"../input/sartorius-cell-instance-segmentation/test\"\n\n\nNUM_EPOCHS = 12\n\nDEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')","metadata":{"execution":{"iopub.status.busy":"2021-10-20T14:28:57.899125Z","iopub.execute_input":"2021-10-20T14:28:57.899406Z","iopub.status.idle":"2021-10-20T14:28:57.941629Z","shell.execute_reply.started":"2021-10-20T14:28:57.89935Z","shell.execute_reply":"2021-10-20T14:28:57.939625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Traning Dataset","metadata":{}},{"cell_type":"markdown","source":"## Utilities","metadata":{}},{"cell_type":"code","source":"def rle_decode(mask_rle, shape, color=1):\n    '''\n    mask_rle: run-length as string formated (start length)\n    shape: (height,width) of array to return \n    Returns numpy array, 1 - mask, 0 - background\n\n    '''\n    s = mask_rle.split()\n    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n    starts -= 1\n    ends = starts + lengths\n    img = np.zeros(shape[0] * shape[1], dtype=np.float32)\n    for lo, hi in zip(starts, ends):\n        img[lo : hi] = color\n    return img.reshape(shape)\n\n\ndef get_transform(train):\n    transforms = []\n    # converts the image, a PIL image, into a PyTorch Tensor\n    transforms.append(ToTensor())\n    if train:\n        # during training, randomly flip the training images\n        # and ground-truth for data augmentation\n        transforms.append(RandomHorizontalFlip(0.5))\n    return Compose(transforms)","metadata":{"execution":{"iopub.status.busy":"2021-10-20T14:28:57.944979Z","iopub.execute_input":"2021-10-20T14:28:57.945431Z","iopub.status.idle":"2021-10-20T14:28:57.954979Z","shell.execute_reply.started":"2021-10-20T14:28:57.945393Z","shell.execute_reply":"2021-10-20T14:28:57.954201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training Dataset and DataLoader","metadata":{}},{"cell_type":"code","source":"class CellDataset(Dataset):\n    def __init__(self, image_dir, df_path, height, width, transforms=None):\n        self.transforms = transforms\n        self.image_dir = image_dir\n        self.df = pd.read_csv(df_path)\n        self.height = height\n        self.width = width\n        self.image_info = collections.defaultdict(dict)\n        temp_df = self.df.groupby('id')['annotation'].agg(lambda x: list(x)).reset_index()\n        for index, row in temp_df.iterrows():\n            self.image_info[index] = {\n                    'image_id': row['id'],\n                    'image_path': os.path.join(self.image_dir, row['id'] + '.png'),\n                    'annotations': row[\"annotation\"]\n                    }\n            \n    def __getitem__(self, idx):\n        # load images ad masks\n        img_path = self.image_info[idx][\"image_path\"]\n        img = Image.open(img_path).convert(\"RGB\")\n        #img = img.resize((self.width, self.height), resample=Image.BILINEAR)\n\n        info = self.image_info[idx]\n\n        mask = np.zeros((len(info['annotations']), self.width, self.height), dtype=np.uint8)\n        labels = []\n        \n        for m, annotation in enumerate(info['annotations']):\n            sub_mask = rle_decode(annotation, (520, 704))\n            sub_mask = Image.fromarray(sub_mask)\n            #sub_mask = sub_mask.resize((self.width, self.height), resample=Image.BILINEAR)\n            sub_mask = np.array(sub_mask) > 0\n            mask[m, :, :] = sub_mask\n            labels.append(1)\n\n        num_objs = len(labels)\n        boxes = []\n        new_labels = []\n        new_masks = []\n\n        for i in range(num_objs):\n            try:\n                pos = np.where(mask[i, :, :])\n                xmin = np.min(pos[1])\n                xmax = np.max(pos[1])\n                ymin = np.min(pos[0])\n                ymax = np.max(pos[0])\n                boxes.append([xmin, ymin, xmax, ymax])\n                new_labels.append(labels[i])\n                new_masks.append(mask[i, :, :])\n            except ValueError:\n                print(\"Error in xmax xmin\")\n                pass\n\n        if len(new_labels) == 0:\n            boxes.append([0, 0, 20, 20])\n            new_labels.append(0)\n            new_masks.append(mask[0, :, :])\n\n        nmx = np.zeros((len(new_masks), self.width, self.height), dtype=np.uint8)\n        for i, n in enumerate(new_masks):\n            nmx[i, :, :] = n\n\n        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n        labels = torch.as_tensor(new_labels, dtype=torch.int64)\n        masks = torch.as_tensor(nmx, dtype=torch.uint8)\n\n        image_id = torch.tensor([idx])\n        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n\n        target = {}\n        target[\"boxes\"] = boxes\n        target[\"labels\"] = labels\n        target[\"masks\"] = masks\n        target[\"image_id\"] = image_id\n        target[\"area\"] = area\n        target[\"iscrowd\"] = iscrowd\n\n        if self.transforms is not None:\n            img, target = self.transforms(img, target)\n\n        return img, target\n\n    def __len__(self):\n        return len(self.image_info)","metadata":{"execution":{"iopub.status.busy":"2021-10-20T14:28:57.958994Z","iopub.execute_input":"2021-10-20T14:28:57.959435Z","iopub.status.idle":"2021-10-20T14:28:57.981194Z","shell.execute_reply.started":"2021-10-20T14:28:57.959385Z","shell.execute_reply":"2021-10-20T14:28:57.980227Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ds_train = CellDataset(TRAIN_PATH, TRAIN_CSV, 704, 520, transforms=get_transform(train=True))\n\ndl_train = DataLoader(ds_train, batch_size=2, shuffle=True, \n                      num_workers=2, collate_fn=lambda x: tuple(zip(*x)))","metadata":{"execution":{"iopub.status.busy":"2021-10-20T14:28:57.98351Z","iopub.execute_input":"2021-10-20T14:28:57.984198Z","iopub.status.idle":"2021-10-20T14:28:58.324654Z","shell.execute_reply.started":"2021-10-20T14:28:57.984155Z","shell.execute_reply":"2021-10-20T14:28:58.323882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train loop","metadata":{}},{"cell_type":"markdown","source":"## Model","metadata":{}},{"cell_type":"code","source":"# Override pythorch checkpoint with an \"offline\" version of the file\n!mkdir -p /root/.cache/torch/hub/checkpoints/\n!cp ../input/cocopre/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth /root/.cache/torch/hub/checkpoints/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth","metadata":{"execution":{"iopub.status.busy":"2021-10-20T14:28:58.326017Z","iopub.execute_input":"2021-10-20T14:28:58.326279Z","iopub.status.idle":"2021-10-20T14:28:59.986597Z","shell.execute_reply.started":"2021-10-20T14:28:58.326244Z","shell.execute_reply":"2021-10-20T14:28:59.985658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_model():\n    NUM_CLASSES = 2 # This is just a dummy value for the classification head\n    \n    model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n\n    # get the number of input features for the classifier\n    in_features = model.roi_heads.box_predictor.cls_score.in_features\n    # replace the pre-trained head with a new one\n    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, NUM_CLASSES)\n\n    # now get the number of input features for the mask classifier\n    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n    hidden_layer = 256\n    # and replace the mask predictor with a new one\n    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask, hidden_layer, NUM_CLASSES)\n    return model\n\n\n# Get the Mask R-CNN model\n# The model does classification, bounding boxes and MASKs for individuals, all at the same time\n# We only care about MASKS\nmodel = get_model()\nmodel.to(DEVICE)\n\nfor param in model.parameters():\n    param.requires_grad = True\n    \nmodel.train();","metadata":{"execution":{"iopub.status.busy":"2021-10-20T14:28:59.988246Z","iopub.execute_input":"2021-10-20T14:28:59.988542Z","iopub.status.idle":"2021-10-20T14:29:02.429401Z","shell.execute_reply.started":"2021-10-20T14:28:59.988496Z","shell.execute_reply":"2021-10-20T14:29:02.428597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training loop!","metadata":{}},{"cell_type":"code","source":"params = [p for p in model.parameters() if p.requires_grad]\noptimizer = torch.optim.SGD(params, lr=0.001, momentum=0.9, weight_decay=0.0005)\n\nn_batches = len(dl_train)\n\nfor epoch in range(1, NUM_EPOCHS + 1):\n    print(f\"Starting epoch {epoch} of {NUM_EPOCHS}\")\n    loss_accum = 0.0\n    loss_mask_accum = 0.0\n    for batch_idx, (images, targets) in enumerate(dl_train, 1):\n    \n        # Predict\n        images = list(image.to(DEVICE) for image in images)\n        targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n\n        loss_dict = model(images, targets)\n        losses = sum(loss for loss in loss_dict.values())\n\n        loss_value = losses.item()\n        loss_mask = loss_dict['loss_mask'].item()\n        # 'loss_classifier', 'loss_box_reg':, 'loss_mask', 'loss_objectness', 'loss_rpn_box_reg'\n        # print(\"Losses: \", {k: v.item() for k, v in loss_dict.items()})\n        \n        # Backprop\n        optimizer.zero_grad()\n        losses.backward()\n        optimizer.step()\n        \n        # Logging\n        loss_accum += loss_value\n        loss_mask_accum += loss_mask\n        if batch_idx % 10 == 0:\n            print(f\"  [Batch {batch_idx:3d} / {n_batches:3d}] Batch train loss: {loss_value:.3f}. Mask-only loss: {loss_mask:.3f}\")\n    \n    epoch_loss = loss_accum / n_batches\n    epoch_mask_loss = loss_mask_accum / n_batches\n    print(f\"[{epoch:2d} / {NUM_EPOCHS:2d}] Train loss: {epoch_loss:.3f}. Train mask-only loss: {epoch_mask_loss:.3f}\")\n\ntorch.save(model.state_dict(), \"model.bin\")","metadata":{"execution":{"iopub.status.busy":"2021-10-20T14:29:02.430752Z","iopub.execute_input":"2021-10-20T14:29:02.431013Z","iopub.status.idle":"2021-10-20T14:30:05.671895Z","shell.execute_reply.started":"2021-10-20T14:29:02.430978Z","shell.execute_reply":"2021-10-20T14:30:05.670279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Analyze prediction results for train set","metadata":{}},{"cell_type":"code","source":"def analyze_train_sample(model, ds_train, sample_index):\n    img, targets = ds_train[sample_index]\n    plt.imshow(img.numpy().transpose((1,2,0)))\n    plt.title(\"Image\")\n    plt.show()\n    \n    all_masks = np.zeros((520, 704))\n    for mask in targets['masks']:\n        all_masks = np.logical_or(all_masks, mask)\n    plt.imshow(img.numpy().transpose((1,2,0)))\n    plt.imshow(all_masks, alpha=0.3)\n    plt.title(\"Ground truth\")\n    plt.show()\n    \n    model.eval()\n    with torch.no_grad():\n        preds = model([img.to(DEVICE)])[0]\n\n    plt.imshow(img.cpu().numpy().transpose((1,2,0)))\n    all_preds_masks = np.zeros((520, 704))\n    for mask in preds['masks'].cpu().detach().numpy():\n        all_preds_masks = np.logical_or(all_preds_masks, mask[0])\n    plt.imshow(all_preds_masks, alpha=0.4)\n    plt.title(\"Predictions\")\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-20T14:30:05.67376Z","iopub.status.idle":"2021-10-20T14:30:05.674464Z","shell.execute_reply.started":"2021-10-20T14:30:05.67418Z","shell.execute_reply":"2021-10-20T14:30:05.674208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# NOTE: It puts the model in eval mode!! Revert for re-training\nanalyze_train_sample(model, ds_train, 20)","metadata":{"execution":{"iopub.status.busy":"2021-10-20T14:30:05.675801Z","iopub.status.idle":"2021-10-20T14:30:05.676418Z","shell.execute_reply.started":"2021-10-20T14:30:05.676148Z","shell.execute_reply":"2021-10-20T14:30:05.676177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"analyze_train_sample(model, ds_train, 100)","metadata":{"execution":{"iopub.status.busy":"2021-10-20T14:30:05.677597Z","iopub.status.idle":"2021-10-20T14:30:05.678209Z","shell.execute_reply.started":"2021-10-20T14:30:05.677955Z","shell.execute_reply":"2021-10-20T14:30:05.677983Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"analyze_train_sample(model, ds_train, 2)","metadata":{"execution":{"iopub.status.busy":"2021-10-20T14:30:05.679359Z","iopub.status.idle":"2021-10-20T14:30:05.679978Z","shell.execute_reply.started":"2021-10-20T14:30:05.679725Z","shell.execute_reply":"2021-10-20T14:30:05.679752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prediction","metadata":{}},{"cell_type":"markdown","source":"## Test Dataset and DataLoader","metadata":{}},{"cell_type":"code","source":"class CellTestDataset(Dataset):\n    def __init__(self, image_dir, height, width, transforms=None):\n        self.transforms = transforms\n        \n        self.image_dir = image_dir\n        \n        self.image_ids = [f[:-4]for f in os.listdir(self.image_dir)]\n        self.num_samples = len(self.image_ids)\n        \n        self.height = height\n        self.width = width\n        self.image_info = collections.defaultdict(dict)\n            \n    def __getitem__(self, idx):\n        image_id = self.image_ids[idx]\n        image_path = os.path.join(self.image_dir, image_id + '.png')\n        image = Image.open(image_path).convert(\"RGB\")\n        #image = image.resize((self.width, self.height), resample=Image.BILINEAR)\n\n        if self.transforms is not None:\n            image, _ = self.transforms(image=image, target=None)\n        return {'image': image, 'image_id': image_id}\n\n    def __len__(self):\n        return len(self.image_ids)","metadata":{"execution":{"iopub.status.busy":"2021-10-20T14:30:05.681131Z","iopub.status.idle":"2021-10-20T14:30:05.681754Z","shell.execute_reply.started":"2021-10-20T14:30:05.681486Z","shell.execute_reply":"2021-10-20T14:30:05.681513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ds_test = CellTestDataset(TEST_PATH, 704, 520, transforms=get_transform(train=False))\nds_test[0]","metadata":{"execution":{"iopub.status.busy":"2021-10-20T14:30:05.682884Z","iopub.status.idle":"2021-10-20T14:30:05.683501Z","shell.execute_reply.started":"2021-10-20T14:30:05.683232Z","shell.execute_reply":"2021-10-20T14:30:05.683258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Utilities","metadata":{}},{"cell_type":"code","source":"# Stolen from: https://www.kaggle.com/arunamenon/cell-instance-segmentation-unet-eda\n# Run-length encoding stolen from https://www.kaggle.com/rakhlin/fast-run-length-encoding-python\n# Modified by me\ndef rle_encoding(x):\n    dots = np.where(x.flatten() == 1)[0]\n    run_lengths = []\n    prev = -2\n    for b in dots:\n        if (b>prev+1): run_lengths.extend((b + 1, 0))\n        run_lengths[-1] += 1\n        prev = b\n    return ' '.join(map(str, run_lengths))\n\n\ndef does_overlap(mask, other_masks):\n    for other_mask in other_masks:\n        if np.sum(np.logical_and(mask, other_mask)) > 0:\n            #import pdb; pdb.set_trace()\n            #print(\"Found overlapping masks!\")\n            return True\n    return False\n\n\ndef remove_overlapping_pixels(mask, other_masks):\n    for other_mask in other_masks:\n        if np.sum(np.logical_and(mask, other_mask)) > 0:\n            mask[np.logical_and(mask, other_mask)] = 0\n    return mask","metadata":{"execution":{"iopub.status.busy":"2021-10-20T14:30:05.684674Z","iopub.status.idle":"2021-10-20T14:30:05.685262Z","shell.execute_reply.started":"2021-10-20T14:30:05.685012Z","shell.execute_reply":"2021-10-20T14:30:05.685037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Submission","metadata":{}},{"cell_type":"code","source":"DROP_OVERLAPPING = False\n\nsublist = []\ncounter = 0\n\nwidth = 704\nheight = 520\nTHRESHOLD = 0.5\n\nmodel.eval()\n\nfor sample in ds_test:\n    img = sample['image']\n    image_id = sample['image_id']\n    with torch.no_grad():\n        result = model([img.to(DEVICE)])[0]\n    if len(result[\"masks\"]) > 0:\n        previous_masks = []\n        for j, m in enumerate(result[\"masks\"]):\n            original_mask = result[\"masks\"][j][0].cpu().numpy()\n            if DROP_OVERLAPPING and does_overlap(original_mask, previous_masks):\n                continue\n            else:\n                original_mask = remove_overlapping_pixels(original_mask, previous_masks)\n                previous_masks.append(original_mask)\n                #img_mask = ToPILImage()(original_mask)\n                #mask = np.asarray(img_mask.resize((width, height), resample=Image.BILINEAR))\n                rle = rle_encoding(original_mask > THRESHOLD)\n                sublist.append([image_id, rle])\n    else:\n        sublist.append([image_id, \"\"])\n\ndf_sub = pd.DataFrame(sublist, columns=['id', 'predicted'])\ndf_sub.to_csv(\"submission.csv\", index=False)\ndf_sub.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-20T14:30:05.686459Z","iopub.status.idle":"2021-10-20T14:30:05.687054Z","shell.execute_reply.started":"2021-10-20T14:30:05.686804Z","shell.execute_reply":"2021-10-20T14:30:05.68683Z"},"trusted":true},"execution_count":null,"outputs":[]}]}